{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and Input.xlsx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>167.0</td>\n",
       "      <td>https://insights.blackcoffer.com/role-big-data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>168.0</td>\n",
       "      <td>https://insights.blackcoffer.com/sales-forecas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>169.0</td>\n",
       "      <td>https://insights.blackcoffer.com/detect-data-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>170.0</td>\n",
       "      <td>https://insights.blackcoffer.com/data-exfiltra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>171.0</td>\n",
       "      <td>https://insights.blackcoffer.com/impacts-of-co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL\n",
       "0       1.0  https://insights.blackcoffer.com/how-is-login-...\n",
       "1       2.0  https://insights.blackcoffer.com/how-does-ai-h...\n",
       "2       3.0  https://insights.blackcoffer.com/ai-and-its-im...\n",
       "3       4.0  https://insights.blackcoffer.com/how-do-deep-l...\n",
       "4       5.0  https://insights.blackcoffer.com/how-artificia...\n",
       "..      ...                                                ...\n",
       "165   167.0  https://insights.blackcoffer.com/role-big-data...\n",
       "166   168.0  https://insights.blackcoffer.com/sales-forecas...\n",
       "167   169.0  https://insights.blackcoffer.com/detect-data-e...\n",
       "168   170.0  https://insights.blackcoffer.com/data-exfiltra...\n",
       "169   171.0  https://insights.blackcoffer.com/impacts-of-co...\n",
       "\n",
       "[170 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "input = pd.read_excel('Input.xlsx')\n",
    "input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping data from web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 96.0.4664\n",
      "Get LATEST chromedriver version for 96.0.4664 google-chrome\n",
      "Driver [C:\\Users\\acer\\.wdm\\drivers\\chromedriver\\win32\\96.0.4664.45\\chromedriver.exe] found in cache\n",
      "C:\\Users\\acer\\AppData\\Local\\Temp/ipykernel_20136/354948620.py:3: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "def scrapper(url):\n",
    "    global driver\n",
    "    driver.get(url)\n",
    "    title=driver.find_element(By.XPATH,\"//div[contains(@class,'td-post-content')]\")\n",
    "    driver.implicitly_wait(10)\n",
    "    return title.text\n",
    "\n",
    "def filesaver(scrapdata):\n",
    "    for data in scrapdata:\n",
    "        name=str(data['URL_ID'])+\".txt\"\n",
    "        \n",
    "        f=open(\"./Articles/\"+name,'w+',encoding='utf-8')\n",
    "        f.write(data['TEXT'])\n",
    "        f.close()\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapdata=[]\n",
    "\n",
    "for index, row in input.iterrows():\n",
    "    element={}\n",
    "    element['URL_ID']=row['URL_ID']\n",
    "    element['TEXT']=scrapper(row['URL'])\n",
    "    scrapdata.append(element)\n",
    "filesaver(scrapdata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving scrapped data into BLACKCOFFER.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "path = 'Articles'\n",
    "files = listdir(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"filename\",\"text\"])\n",
    "for filename in files:\n",
    "    f= open(\"./Articles/\"+filename,\"r\",encoding='utf-8')\n",
    "    text = f.read()\n",
    "    sr = int(filename.replace(\".0.txt\",\"\"))\n",
    "\n",
    "    df = df.append({\"filename\":sr,\"text\":text},ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(\"filename\")\n",
    "df\n",
    "df.to_csv(\"BLACKCOFFER.csv\",index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>When people hear AI they often think about sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>With increasing computing power and more data,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>If you were a fan of the 90’s film Clueless ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Understanding exactly how data is ingested, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>From the stone age to the modern world, from h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>167</td>\n",
       "      <td>CAN ACADEMIA, RESEARCHERS, DECISION MAKERS AND...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>168</td>\n",
       "      <td>Introduction\\nInventory planning is a fundamen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>169</td>\n",
       "      <td>1. The Problem\\nInsider threat detection speci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>170</td>\n",
       "      <td>If we talk in terms of our general life, Exfil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>171</td>\n",
       "      <td>Some vendors (fruit and vegetable sellers) beg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     filename                                               text\n",
       "0           1  When people hear AI they often think about sen...\n",
       "1           2  With increasing computing power and more data,...\n",
       "2           3  If you were a fan of the 90’s film Clueless ba...\n",
       "3           4  Understanding exactly how data is ingested, an...\n",
       "4           5  From the stone age to the modern world, from h...\n",
       "..        ...                                                ...\n",
       "165       167  CAN ACADEMIA, RESEARCHERS, DECISION MAKERS AND...\n",
       "166       168  Introduction\\nInventory planning is a fundamen...\n",
       "167       169  1. The Problem\\nInsider threat detection speci...\n",
       "168       170  If we talk in terms of our general life, Exfil...\n",
       "169       171  Some vendors (fruit and vegetable sellers) beg...\n",
       "\n",
       "[170 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('BLACKCOFFER.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"NO. OF SENTENCES\"]=df['text'].apply(lambda x: len(x.split('.')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating custom stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "path = 'Stop Words'\n",
    "files = listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords=[]\n",
    "for filename in files:\n",
    "    f= open(\"./Stop Words/\"+filename,\"r\")\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.replace(\"\\n\",\"\")\n",
    "        line = line.split(\"|\")\n",
    "        \n",
    "\n",
    "        if isinstance(line,list):\n",
    "            for l in line:\n",
    "                l=l.strip()\n",
    "                custom_stopwords.append(l)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            custom_stopwords.append(line)\n",
    "            \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict_contractions():    \n",
    "    return {\n",
    "        \"cant\":\"can not\",\n",
    "        \"dont\":\"do not\",\n",
    "        \"wont\":\"will not\",\n",
    "        \"ain't\":\"is not\",\n",
    "        \"amn't\":\"am not\",\n",
    "        \"aren't\":\"are not\",\n",
    "        \"can't\":\"cannot\",\n",
    "        \"'cause\":\"because\",\n",
    "        \"couldn't\":\"could not\",\n",
    "        \"couldn't've\":\"could not have\",\n",
    "        \"could've\":\"could have\",\n",
    "        \"daren't\":\"dare not\",\n",
    "        \"daresn't\":\"dare not\",\n",
    "        \"dasn't\":\"dare not\",\n",
    "        \"didn't\":\"did not\",\n",
    "        \"doesn't\":\"does not\",\n",
    "        \"don't\":\"do not\",\n",
    "        \"e'er\":\"ever\",\n",
    "        \"em\":\"them\",\n",
    "        \"everyone's\":\"everyone is\",\n",
    "        \"finna\":\"fixing to\",\n",
    "        \"gimme\":\"give me\",\n",
    "        \"gonna\":\"going to\",\n",
    "        \"gon't\":\"go not\",\n",
    "        \"gotta\":\"got to\",\n",
    "        \"hadn't\":\"had not\",\n",
    "        \"hasn't\":\"has not\",\n",
    "        \"haven't\":\"have not\",\n",
    "        \"he'd\":\"he would\",\n",
    "        \"he'll\":\"he will\",\n",
    "        \"he's\":\"he is\",\n",
    "        \"he've\":\"he have\",\n",
    "        \"how'd\":\"how would\",\n",
    "        \"how'll\":\"how will\",\n",
    "        \"how're\":\"how are\",\n",
    "        \"how's\":\"how is\",\n",
    "        \"I'd\":\"I would\",\n",
    "        \"I'll\":\"I will\",\n",
    "        \"I'm\":\"I am\",\n",
    "        \"I'm'a\":\"I am about to\",\n",
    "        \"I'm'o\":\"I am going to\",\n",
    "        \"isn't\":\"is not\",\n",
    "        \"it'd\":\"it would\",\n",
    "        \"it'll\":\"it will\",\n",
    "        \"it's\":\"it is\",\n",
    "        \"I've\":\"I have\",\n",
    "        \"kinda\":\"kind of\",\n",
    "        \"let's\":\"let us\",\n",
    "        \"mayn't\":\"may not\",\n",
    "        \"may've\":\"may have\",\n",
    "        \"mightn't\":\"might not\",\n",
    "        \"might've\":\"might have\",\n",
    "        \"mustn't\":\"must not\",\n",
    "        \"mustn't've\":\"must not have\",\n",
    "        \"must've\":\"must have\",\n",
    "        \"needn't\":\"need not\",\n",
    "        \"ne'er\":\"never\",\n",
    "        \"o'\":\"of\",\n",
    "        \"o'er\":\"over\",\n",
    "        \"ol'\":\"old\",\n",
    "        \"oughtn't\":\"ought not\",\n",
    "        \"shalln't\":\"shall not\",\n",
    "        \"shan't\":\"shall not\",\n",
    "        \"she'd\":\"she would\",\n",
    "        \"she'll\":\"she will\",\n",
    "        \"she's\":\"she is\",\n",
    "        \"shouldn't\":\"should not\",\n",
    "        \"shouldn't've\":\"should not have\",\n",
    "        \"should've\":\"should have\",\n",
    "        \"somebody's\":\"somebody is\",\n",
    "        \"someone's\":\"someone is\",\n",
    "        \"something's\":\"something is\",\n",
    "        \"that'd\":\"that would\",\n",
    "        \"that'll\":\"that will\",\n",
    "        \"that're\":\"that are\",\n",
    "        \"that's\":\"that is\",\n",
    "        \"there'd\":\"there would\",\n",
    "        \"there'll\":\"there will\",\n",
    "        \"there're\":\"there are\",\n",
    "        \"there's\":\"there is\",\n",
    "        \"these're\":\"these are\",\n",
    "        \"they'd\":\"they would\",\n",
    "        \"they'll\":\"they will\",\n",
    "        \"they're\":\"they are\",\n",
    "        \"they've\":\"they have\",\n",
    "        \"this's\":\"this is\",\n",
    "        \"those're\":\"those are\",\n",
    "        \"'tis\":\"it is\",\n",
    "        \"'twas\":\"it was\",\n",
    "        \"wanna\":\"want to\",\n",
    "        \"wasn't\":\"was not\",\n",
    "        \"we'd\":\"we would\",\n",
    "        \"we'd've\":\"we would have\",\n",
    "        \"we'll\":\"we will\",\n",
    "        \"we're\":\"we are\",\n",
    "        \"weren't\":\"were not\",\n",
    "        \"we've\":\"we have\",\n",
    "        \"what'd\":\"what did\",\n",
    "        \"what'll\":\"what will\",\n",
    "        \"what're\":\"what are\",\n",
    "        \"what's\":\"what is\",\n",
    "        \"what've\":\"what have\",\n",
    "        \"when's\":\"when is\",\n",
    "        \"where'd\":\"where did\",\n",
    "        \"where're\":\"where are\",\n",
    "        \"where's\":\"where is\",\n",
    "        \"where've\":\"where have\",\n",
    "        \"which's\":\"which is\",\n",
    "        \"who'd\":\"who would\",\n",
    "        \"who'd've\":\"who would have\",\n",
    "        \"who'll\":\"who will\",\n",
    "        \"who're\":\"who are\",\n",
    "        \"who's\":\"who is\",\n",
    "        \"who've\":\"who have\",\n",
    "        \"why'd\":\"why did\",\n",
    "        \"why're\":\"why are\",\n",
    "        \"why's\":\"why is\",\n",
    "        \"won't\":\"will not\",\n",
    "        \"wouldn't\":\"would not\",\n",
    "        \"would've\":\"would have\",\n",
    "        \"y'all\":\"you all\",\n",
    "        \"you'd\":\"you would\",\n",
    "        \"you'll\":\"you will\",\n",
    "        \"you're\":\"you are\",\n",
    "        \"you've\":\"you have\",\n",
    "        \"Whatcha\":\"What are you\",\n",
    "        \"luv\":\"love\",\n",
    "        \"sux\":\"sucks\",\n",
    "        \"couldn't\":\"could not\",\n",
    "        \"wouldn't\":\"would not\",\n",
    "        \"shouldn't\":\"should not\",\n",
    "        \"im\":\"i am\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "def normalization(text):\n",
    "    text = str(text).lower()\n",
    "    # URL\n",
    "    text = re.sub('((www.[^\\s]+)|(https?://[^\\s]+))',' ',text)\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "\n",
    "    # Number\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "    # Punctuation\n",
    "    #text = ' '.join([char for char in text if char not in string.punctuation])\n",
    "    for sym in string.punctuation:\n",
    "        text = text.replace(sym, \" \")\n",
    "    CONTRACTIONS = load_dict_contractions()\n",
    "    text = text.replace(\"’\",\"'\")\n",
    "    words = text.split()\n",
    "    reformed = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in words]\n",
    "    text = \" \".join(reformed)\n",
    "    return text\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(normalization)\n",
    "df['clean_text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (custom_stopwords)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df['clean_text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Seq_num</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Word Proportion</th>\n",
       "      <th>Average Proportion</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Doc Count</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Uncertainty</th>\n",
       "      <th>Litigious</th>\n",
       "      <th>Strong_Modal</th>\n",
       "      <th>Weak_Modal</th>\n",
       "      <th>Constraining</th>\n",
       "      <th>Complexity</th>\n",
       "      <th>Syllables</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AARDVARK</td>\n",
       "      <td>1</td>\n",
       "      <td>312</td>\n",
       "      <td>1.422050e-08</td>\n",
       "      <td>1.335201e-08</td>\n",
       "      <td>3.700747e-06</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AARDVARKS</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.367356e-10</td>\n",
       "      <td>8.882163e-12</td>\n",
       "      <td>9.362849e-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABACI</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>4.102067e-10</td>\n",
       "      <td>1.200533e-10</td>\n",
       "      <td>5.359747e-08</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABACK</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>6.836779e-10</td>\n",
       "      <td>4.080549e-10</td>\n",
       "      <td>1.406914e-07</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABACUS</td>\n",
       "      <td>5</td>\n",
       "      <td>8009</td>\n",
       "      <td>3.650384e-07</td>\n",
       "      <td>3.798698e-07</td>\n",
       "      <td>3.523914e-05</td>\n",
       "      <td>1058</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86526</th>\n",
       "      <td>ZYGOTE</td>\n",
       "      <td>86529</td>\n",
       "      <td>48</td>\n",
       "      <td>2.187769e-09</td>\n",
       "      <td>8.817180e-10</td>\n",
       "      <td>1.907714e-07</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86527</th>\n",
       "      <td>ZYGOTES</td>\n",
       "      <td>86530</td>\n",
       "      <td>1</td>\n",
       "      <td>4.557853e-11</td>\n",
       "      <td>1.857263e-11</td>\n",
       "      <td>1.957775e-08</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86528</th>\n",
       "      <td>ZYGOTIC</td>\n",
       "      <td>86531</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86529</th>\n",
       "      <td>ZYMURGIES</td>\n",
       "      <td>86532</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86530</th>\n",
       "      <td>ZYMURGY</td>\n",
       "      <td>86533</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86531 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  Seq_num  Word Count  Word Proportion  Average Proportion  \\\n",
       "0       AARDVARK        1         312     1.422050e-08        1.335201e-08   \n",
       "1      AARDVARKS        2           3     1.367356e-10        8.882163e-12   \n",
       "2          ABACI        3           9     4.102067e-10        1.200533e-10   \n",
       "3          ABACK        4          15     6.836779e-10        4.080549e-10   \n",
       "4         ABACUS        5        8009     3.650384e-07        3.798698e-07   \n",
       "...          ...      ...         ...              ...                 ...   \n",
       "86526     ZYGOTE    86529          48     2.187769e-09        8.817180e-10   \n",
       "86527    ZYGOTES    86530           1     4.557853e-11        1.857263e-11   \n",
       "86528    ZYGOTIC    86531           0     0.000000e+00        0.000000e+00   \n",
       "86529  ZYMURGIES    86532           0     0.000000e+00        0.000000e+00   \n",
       "86530    ZYMURGY    86533           0     0.000000e+00        0.000000e+00   \n",
       "\n",
       "            Std Dev  Doc Count  Negative  Positive  Uncertainty  Litigious  \\\n",
       "0      3.700747e-06         96         0         0            0          0   \n",
       "1      9.362849e-09          1         0         0            0          0   \n",
       "2      5.359747e-08          7         0         0            0          0   \n",
       "3      1.406914e-07         14         0         0            0          0   \n",
       "4      3.523914e-05       1058         0         0            0          0   \n",
       "...             ...        ...       ...       ...          ...        ...   \n",
       "86526  1.907714e-07         33         0         0            0          0   \n",
       "86527  1.957775e-08          1         0         0            0          0   \n",
       "86528  0.000000e+00          0         0         0            0          0   \n",
       "86529  0.000000e+00          0         0         0            0          0   \n",
       "86530  0.000000e+00          0         0         0            0          0   \n",
       "\n",
       "       Strong_Modal  Weak_Modal  Constraining  Complexity  Syllables  \\\n",
       "0                 0           0             0           0          2   \n",
       "1                 0           0             0           0          2   \n",
       "2                 0           0             0           0          3   \n",
       "3                 0           0             0           0          2   \n",
       "4                 0           0             0           0          3   \n",
       "...             ...         ...           ...         ...        ...   \n",
       "86526             0           0             0           0          2   \n",
       "86527             0           0             0           0          2   \n",
       "86528             0           0             0           0          3   \n",
       "86529             0           0             0           0          3   \n",
       "86530             0           0             0           0          3   \n",
       "\n",
       "          Source  \n",
       "0      12of12inf  \n",
       "1      12of12inf  \n",
       "2      12of12inf  \n",
       "3      12of12inf  \n",
       "4      12of12inf  \n",
       "...          ...  \n",
       "86526  12of12inf  \n",
       "86527  12of12inf  \n",
       "86528  12of12inf  \n",
       "86529  12of12inf  \n",
       "86530  12of12inf  \n",
       "\n",
       "[86531 rows x 17 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_dict=pd.read_csv('LoughranMcDonald_MasterDictionary_2020.csv')\n",
    "master_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [] \n",
    "neg =[]\n",
    "Uncertain = []\n",
    "for index,row in master_dict.iterrows():\n",
    "    if row['Negative']>0:\n",
    "        neg.append(row['Word'].lower())\n",
    "    elif row['Positive']>0:\n",
    "        pos.append(row['Word'].lower())\n",
    "    elif row['Uncertainty']>0:\n",
    "        Uncertain.append(row['Word'].lower())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in df.iterrows():\n",
    "    text = row['clean_text']\n",
    "    row['POSITIVE SCORE']=0\n",
    "    row['NEGATIVE SCORE']=0\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word in pos:\n",
    "            row['POSITIVE SCORE']+=1\n",
    "        elif word in neg:\n",
    "            row['NEGATIVE SCORE']+=1\n",
    "    # row['POLARITY SCORE']=(row['POSITIVE SCORE']-row['NEGATIVE SCORE'])/ ((row['POSITIVE SCORE'] + row['NEGATIVE SCORE']) + 0.000001)\n",
    "    # row['SUBJECTIVITY SCORE']=(row['POSITIVE SCORE'] + row['NEGATIVE SCORE'])/ ((len(words)) + 0.000001)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getposscore(text):\n",
    "    score = 0\n",
    "    global pos\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word in pos:\n",
    "            score +=1\n",
    "    return score\n",
    "    \n",
    "def getnegscore(text):\n",
    "    score = 0\n",
    "    global neg\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word in neg:\n",
    "            score +=1\n",
    "    return score\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['POSITIVE SCORE']=df['clean_text'].apply(getposscore)\n",
    "df['NEGATIVE SCORE']=df['clean_text'].apply(getnegscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['POLARITY SCORE']=(df['POSITIVE SCORE']-df['NEGATIVE SCORE'])/ ((df['POSITIVE SCORE'] + df['NEGATIVE SCORE']) + 0.000001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WORD COUNT']=df['clean_text'].apply(lambda x:len(x.split()))\n",
    "df['SUBJECTIVITY SCORE']=(df['POSITIVE SCORE'] + df['NEGATIVE SCORE'])/ ((df['WORD COUNT']) + 0.000001)\n",
    "df['AVG SENTENCE LENGTH']=df['WORD COUNT']/df['NO. OF SENTENCES']\n",
    "df['AVG NUMBER OF WORDS PER SENTENCE'] = df['WORD COUNT']/df['NO. OF SENTENCES']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syllables\n",
    "def countcomplex(text):\n",
    "    count=0\n",
    "    words= text.split(' ')\n",
    "    for word in words:\n",
    "        Syllable =syllables.estimate(word)\n",
    "        \n",
    "        if Syllable >=2:\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "def totalsyllable(text):\n",
    "    count=0\n",
    "    words= text.split(' ')\n",
    "    for word in words:\n",
    "        count+=syllables.estimate(word)\n",
    "    result=count/len(words)\n",
    "    return result\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgwordlength(text):\n",
    "    words = text.split()\n",
    "    no_of_words=len(words)\n",
    "    total_char=0\n",
    "    for word in words:\n",
    "        total_char+=len(word)\n",
    "    return total_char/no_of_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_personal_pronoun(text):\n",
    "    regex = r\"(\\b(s?i|me|we|my|ours|us|I|Me|We|My|Ours|Us)\\b)\"\n",
    "    result = 0\n",
    "\n",
    "    matches = re.finditer(regex,text,re.MULTILINE)\n",
    "    for nummatch,match in enumerate(matches):\n",
    "        result+=1\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>NO. OF SENTENCES</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>when people hear ai they often think about sen...</td>\n",
       "      <td>28</td>\n",
       "      <td>people hear ai sentient robots magic boxes ai ...</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.555555</td>\n",
       "      <td>362</td>\n",
       "      <td>0.024862</td>\n",
       "      <td>12.928571</td>\n",
       "      <td>12.928571</td>\n",
       "      <td>286</td>\n",
       "      <td>79.005525</td>\n",
       "      <td>36.773639</td>\n",
       "      <td>2.317680</td>\n",
       "      <td>6.870166</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>with increasing computing power and more data ...</td>\n",
       "      <td>29</td>\n",
       "      <td>increasing computing power data potential algo...</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>337</td>\n",
       "      <td>0.038576</td>\n",
       "      <td>11.620690</td>\n",
       "      <td>11.620690</td>\n",
       "      <td>272</td>\n",
       "      <td>80.712166</td>\n",
       "      <td>36.933142</td>\n",
       "      <td>2.448071</td>\n",
       "      <td>7.062315</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>if you were a fan of the 's film clueless back...</td>\n",
       "      <td>77</td>\n",
       "      <td>fan 's film clueless back day remember protago...</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>983</td>\n",
       "      <td>0.049847</td>\n",
       "      <td>12.766234</td>\n",
       "      <td>12.766234</td>\n",
       "      <td>783</td>\n",
       "      <td>79.654120</td>\n",
       "      <td>36.968142</td>\n",
       "      <td>2.451679</td>\n",
       "      <td>7.080366</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>understanding exactly how data is ingested ana...</td>\n",
       "      <td>15</td>\n",
       "      <td>understanding data ingested analyzed returned ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>224</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>14.933333</td>\n",
       "      <td>14.933333</td>\n",
       "      <td>182</td>\n",
       "      <td>81.250000</td>\n",
       "      <td>38.473333</td>\n",
       "      <td>2.531250</td>\n",
       "      <td>7.299107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>from the stone age to the modern world from hu...</td>\n",
       "      <td>60</td>\n",
       "      <td>stone age modern world hunting gathering culti...</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>601</td>\n",
       "      <td>0.061564</td>\n",
       "      <td>10.016667</td>\n",
       "      <td>10.016667</td>\n",
       "      <td>449</td>\n",
       "      <td>74.708819</td>\n",
       "      <td>33.890194</td>\n",
       "      <td>2.381032</td>\n",
       "      <td>6.866889</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>167</td>\n",
       "      <td>can academia researchers decision makers and p...</td>\n",
       "      <td>67</td>\n",
       "      <td>academia researchers decision makers policy ma...</td>\n",
       "      <td>13</td>\n",
       "      <td>39</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>707</td>\n",
       "      <td>0.073550</td>\n",
       "      <td>10.552239</td>\n",
       "      <td>10.552239</td>\n",
       "      <td>586</td>\n",
       "      <td>82.885431</td>\n",
       "      <td>37.375068</td>\n",
       "      <td>2.473833</td>\n",
       "      <td>7.152758</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>168</td>\n",
       "      <td>introduction inventory planning is a fundament...</td>\n",
       "      <td>43</td>\n",
       "      <td>introduction inventory planning fundamental pa...</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>549</td>\n",
       "      <td>0.058288</td>\n",
       "      <td>12.767442</td>\n",
       "      <td>12.767442</td>\n",
       "      <td>429</td>\n",
       "      <td>78.142077</td>\n",
       "      <td>36.363807</td>\n",
       "      <td>2.444444</td>\n",
       "      <td>7.222222</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>169</td>\n",
       "      <td>the problem insider threat detection specifica...</td>\n",
       "      <td>63</td>\n",
       "      <td>problem insider threat detection specifically ...</td>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>-0.880000</td>\n",
       "      <td>561</td>\n",
       "      <td>0.089127</td>\n",
       "      <td>8.904762</td>\n",
       "      <td>8.904762</td>\n",
       "      <td>487</td>\n",
       "      <td>86.809269</td>\n",
       "      <td>38.285612</td>\n",
       "      <td>2.554367</td>\n",
       "      <td>7.140820</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>170</td>\n",
       "      <td>if we talk in terms of our general life exfilt...</td>\n",
       "      <td>31</td>\n",
       "      <td>talk terms general life exfiltrate means surre...</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>268</td>\n",
       "      <td>0.052239</td>\n",
       "      <td>8.645161</td>\n",
       "      <td>8.645161</td>\n",
       "      <td>204</td>\n",
       "      <td>76.119403</td>\n",
       "      <td>33.905826</td>\n",
       "      <td>2.462687</td>\n",
       "      <td>7.085821</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>171</td>\n",
       "      <td>some vendors fruit and vegetable sellers began...</td>\n",
       "      <td>27</td>\n",
       "      <td>vendors fruit vegetable sellers began venturin...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>385</td>\n",
       "      <td>0.023377</td>\n",
       "      <td>14.259259</td>\n",
       "      <td>14.259259</td>\n",
       "      <td>308</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>37.703704</td>\n",
       "      <td>2.358442</td>\n",
       "      <td>7.020779</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     filename                                               text  \\\n",
       "0           1  when people hear ai they often think about sen...   \n",
       "1           2  with increasing computing power and more data ...   \n",
       "2           3  if you were a fan of the 's film clueless back...   \n",
       "3           4  understanding exactly how data is ingested ana...   \n",
       "4           5  from the stone age to the modern world from hu...   \n",
       "..        ...                                                ...   \n",
       "165       167  can academia researchers decision makers and p...   \n",
       "166       168  introduction inventory planning is a fundament...   \n",
       "167       169  the problem insider threat detection specifica...   \n",
       "168       170  if we talk in terms of our general life exfilt...   \n",
       "169       171  some vendors fruit and vegetable sellers began...   \n",
       "\n",
       "     NO. OF SENTENCES                                         clean_text  \\\n",
       "0                  28  people hear ai sentient robots magic boxes ai ...   \n",
       "1                  29  increasing computing power data potential algo...   \n",
       "2                  77  fan 's film clueless back day remember protago...   \n",
       "3                  15  understanding data ingested analyzed returned ...   \n",
       "4                  60  stone age modern world hunting gathering culti...   \n",
       "..                ...                                                ...   \n",
       "165                67  academia researchers decision makers policy ma...   \n",
       "166                43  introduction inventory planning fundamental pa...   \n",
       "167                63  problem insider threat detection specifically ...   \n",
       "168                31  talk terms general life exfiltrate means surre...   \n",
       "169                27  vendors fruit vegetable sellers began venturin...   \n",
       "\n",
       "     POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  WORD COUNT  \\\n",
       "0                 2               7       -0.555555         362   \n",
       "1                 7               6        0.076923         337   \n",
       "2                30              19        0.224490         983   \n",
       "3                 4               1        0.600000         224   \n",
       "4                20              17        0.081081         601   \n",
       "..              ...             ...             ...         ...   \n",
       "165              13              39       -0.500000         707   \n",
       "166              20              12        0.250000         549   \n",
       "167               3              47       -0.880000         561   \n",
       "168               4              10       -0.428571         268   \n",
       "169               0               9       -1.000000         385   \n",
       "\n",
       "     SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0              0.024862            12.928571   \n",
       "1              0.038576            11.620690   \n",
       "2              0.049847            12.766234   \n",
       "3              0.022321            14.933333   \n",
       "4              0.061564            10.016667   \n",
       "..                  ...                  ...   \n",
       "165            0.073550            10.552239   \n",
       "166            0.058288            12.767442   \n",
       "167            0.089127             8.904762   \n",
       "168            0.052239             8.645161   \n",
       "169            0.023377            14.259259   \n",
       "\n",
       "     AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  \\\n",
       "0                           12.928571                 286   \n",
       "1                           11.620690                 272   \n",
       "2                           12.766234                 783   \n",
       "3                           14.933333                 182   \n",
       "4                           10.016667                 449   \n",
       "..                                ...                 ...   \n",
       "165                         10.552239                 586   \n",
       "166                         12.767442                 429   \n",
       "167                          8.904762                 487   \n",
       "168                          8.645161                 204   \n",
       "169                         14.259259                 308   \n",
       "\n",
       "     PERCENTAGE OF COMPLEX WORDS  FOG INDEX  SYLLABLE PER WORD  \\\n",
       "0                      79.005525  36.773639           2.317680   \n",
       "1                      80.712166  36.933142           2.448071   \n",
       "2                      79.654120  36.968142           2.451679   \n",
       "3                      81.250000  38.473333           2.531250   \n",
       "4                      74.708819  33.890194           2.381032   \n",
       "..                           ...        ...                ...   \n",
       "165                    82.885431  37.375068           2.473833   \n",
       "166                    78.142077  36.363807           2.444444   \n",
       "167                    86.809269  38.285612           2.554367   \n",
       "168                    76.119403  33.905826           2.462687   \n",
       "169                    80.000000  37.703704           2.358442   \n",
       "\n",
       "     AVG WORD LENGTH  PERSONAL PRONOUNS  \n",
       "0           6.870166                  4  \n",
       "1           7.062315                  2  \n",
       "2           7.080366                 13  \n",
       "3           7.299107                  1  \n",
       "4           6.866889                 28  \n",
       "..               ...                ...  \n",
       "165         7.152758                 15  \n",
       "166         7.222222                  0  \n",
       "167         7.140820                  6  \n",
       "168         7.085821                 11  \n",
       "169         7.020779                  1  \n",
       "\n",
       "[170 rows x 17 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df['COMPLEX WORD COUNT']=df['clean_text'].apply(countcomplex)\n",
    "df['SYLLABLE PER WORD']=df['clean_text'].apply(totalsyllable)\n",
    "df['AVG WORD LENGTH']=df['clean_text'].apply(avgwordlength)\n",
    "df['PERCENTAGE OF COMPLEX WORDS']=(df['COMPLEX WORD COUNT']/df['WORD COUNT'])*100\n",
    "df['FOG INDEX']= 0.4 * (df['AVG SENTENCE LENGTH'] + df['PERCENTAGE OF COMPLEX WORDS'])\n",
    "df['PERSONAL PRONOUNS']=df['text'].apply(get_personal_pronoun)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('processed.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>URL</th>\n",
       "      <th>URL_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.555555</td>\n",
       "      <td>362</td>\n",
       "      <td>0.024862</td>\n",
       "      <td>12.928571</td>\n",
       "      <td>12.928571</td>\n",
       "      <td>286</td>\n",
       "      <td>79.005525</td>\n",
       "      <td>36.773639</td>\n",
       "      <td>2.317680</td>\n",
       "      <td>6.870166</td>\n",
       "      <td>4</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>337</td>\n",
       "      <td>0.038576</td>\n",
       "      <td>11.620690</td>\n",
       "      <td>11.620690</td>\n",
       "      <td>272</td>\n",
       "      <td>80.712166</td>\n",
       "      <td>36.933142</td>\n",
       "      <td>2.448071</td>\n",
       "      <td>7.062315</td>\n",
       "      <td>2</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>983</td>\n",
       "      <td>0.049847</td>\n",
       "      <td>12.766234</td>\n",
       "      <td>12.766234</td>\n",
       "      <td>783</td>\n",
       "      <td>79.654120</td>\n",
       "      <td>36.968142</td>\n",
       "      <td>2.451679</td>\n",
       "      <td>7.080366</td>\n",
       "      <td>13</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>224</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>14.933333</td>\n",
       "      <td>14.933333</td>\n",
       "      <td>182</td>\n",
       "      <td>81.250000</td>\n",
       "      <td>38.473333</td>\n",
       "      <td>2.531250</td>\n",
       "      <td>7.299107</td>\n",
       "      <td>1</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>601</td>\n",
       "      <td>0.061564</td>\n",
       "      <td>10.016667</td>\n",
       "      <td>10.016667</td>\n",
       "      <td>449</td>\n",
       "      <td>74.708819</td>\n",
       "      <td>33.890194</td>\n",
       "      <td>2.381032</td>\n",
       "      <td>6.866889</td>\n",
       "      <td>28</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>13</td>\n",
       "      <td>39</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>707</td>\n",
       "      <td>0.073550</td>\n",
       "      <td>10.552239</td>\n",
       "      <td>10.552239</td>\n",
       "      <td>586</td>\n",
       "      <td>82.885431</td>\n",
       "      <td>37.375068</td>\n",
       "      <td>2.473833</td>\n",
       "      <td>7.152758</td>\n",
       "      <td>15</td>\n",
       "      <td>https://insights.blackcoffer.com/role-big-data...</td>\n",
       "      <td>167.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>549</td>\n",
       "      <td>0.058288</td>\n",
       "      <td>12.767442</td>\n",
       "      <td>12.767442</td>\n",
       "      <td>429</td>\n",
       "      <td>78.142077</td>\n",
       "      <td>36.363807</td>\n",
       "      <td>2.444444</td>\n",
       "      <td>7.222222</td>\n",
       "      <td>0</td>\n",
       "      <td>https://insights.blackcoffer.com/sales-forecas...</td>\n",
       "      <td>168.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>-0.880000</td>\n",
       "      <td>561</td>\n",
       "      <td>0.089127</td>\n",
       "      <td>8.904762</td>\n",
       "      <td>8.904762</td>\n",
       "      <td>487</td>\n",
       "      <td>86.809269</td>\n",
       "      <td>38.285612</td>\n",
       "      <td>2.554367</td>\n",
       "      <td>7.140820</td>\n",
       "      <td>6</td>\n",
       "      <td>https://insights.blackcoffer.com/detect-data-e...</td>\n",
       "      <td>169.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>268</td>\n",
       "      <td>0.052239</td>\n",
       "      <td>8.645161</td>\n",
       "      <td>8.645161</td>\n",
       "      <td>204</td>\n",
       "      <td>76.119403</td>\n",
       "      <td>33.905826</td>\n",
       "      <td>2.462687</td>\n",
       "      <td>7.085821</td>\n",
       "      <td>11</td>\n",
       "      <td>https://insights.blackcoffer.com/data-exfiltra...</td>\n",
       "      <td>170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>385</td>\n",
       "      <td>0.023377</td>\n",
       "      <td>14.259259</td>\n",
       "      <td>14.259259</td>\n",
       "      <td>308</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>37.703704</td>\n",
       "      <td>2.358442</td>\n",
       "      <td>7.020779</td>\n",
       "      <td>1</td>\n",
       "      <td>https://insights.blackcoffer.com/impacts-of-co...</td>\n",
       "      <td>171.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  WORD COUNT  \\\n",
       "0                 2               7       -0.555555         362   \n",
       "1                 7               6        0.076923         337   \n",
       "2                30              19        0.224490         983   \n",
       "3                 4               1        0.600000         224   \n",
       "4                20              17        0.081081         601   \n",
       "..              ...             ...             ...         ...   \n",
       "165              13              39       -0.500000         707   \n",
       "166              20              12        0.250000         549   \n",
       "167               3              47       -0.880000         561   \n",
       "168               4              10       -0.428571         268   \n",
       "169               0               9       -1.000000         385   \n",
       "\n",
       "     SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0              0.024862            12.928571   \n",
       "1              0.038576            11.620690   \n",
       "2              0.049847            12.766234   \n",
       "3              0.022321            14.933333   \n",
       "4              0.061564            10.016667   \n",
       "..                  ...                  ...   \n",
       "165            0.073550            10.552239   \n",
       "166            0.058288            12.767442   \n",
       "167            0.089127             8.904762   \n",
       "168            0.052239             8.645161   \n",
       "169            0.023377            14.259259   \n",
       "\n",
       "     AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  \\\n",
       "0                           12.928571                 286   \n",
       "1                           11.620690                 272   \n",
       "2                           12.766234                 783   \n",
       "3                           14.933333                 182   \n",
       "4                           10.016667                 449   \n",
       "..                                ...                 ...   \n",
       "165                         10.552239                 586   \n",
       "166                         12.767442                 429   \n",
       "167                          8.904762                 487   \n",
       "168                          8.645161                 204   \n",
       "169                         14.259259                 308   \n",
       "\n",
       "     PERCENTAGE OF COMPLEX WORDS  FOG INDEX  SYLLABLE PER WORD  \\\n",
       "0                      79.005525  36.773639           2.317680   \n",
       "1                      80.712166  36.933142           2.448071   \n",
       "2                      79.654120  36.968142           2.451679   \n",
       "3                      81.250000  38.473333           2.531250   \n",
       "4                      74.708819  33.890194           2.381032   \n",
       "..                           ...        ...                ...   \n",
       "165                    82.885431  37.375068           2.473833   \n",
       "166                    78.142077  36.363807           2.444444   \n",
       "167                    86.809269  38.285612           2.554367   \n",
       "168                    76.119403  33.905826           2.462687   \n",
       "169                    80.000000  37.703704           2.358442   \n",
       "\n",
       "     AVG WORD LENGTH  PERSONAL PRONOUNS  \\\n",
       "0           6.870166                  4   \n",
       "1           7.062315                  2   \n",
       "2           7.080366                 13   \n",
       "3           7.299107                  1   \n",
       "4           6.866889                 28   \n",
       "..               ...                ...   \n",
       "165         7.152758                 15   \n",
       "166         7.222222                  0   \n",
       "167         7.140820                  6   \n",
       "168         7.085821                 11   \n",
       "169         7.020779                  1   \n",
       "\n",
       "                                                   URL  URL_ID  \n",
       "0    https://insights.blackcoffer.com/how-is-login-...     1.0  \n",
       "1    https://insights.blackcoffer.com/how-does-ai-h...     2.0  \n",
       "2    https://insights.blackcoffer.com/ai-and-its-im...     3.0  \n",
       "3    https://insights.blackcoffer.com/how-do-deep-l...     4.0  \n",
       "4    https://insights.blackcoffer.com/how-artificia...     5.0  \n",
       "..                                                 ...     ...  \n",
       "165  https://insights.blackcoffer.com/role-big-data...   167.0  \n",
       "166  https://insights.blackcoffer.com/sales-forecas...   168.0  \n",
       "167  https://insights.blackcoffer.com/detect-data-e...   169.0  \n",
       "168  https://insights.blackcoffer.com/data-exfiltra...   170.0  \n",
       "169  https://insights.blackcoffer.com/impacts-of-co...   171.0  \n",
       "\n",
       "[170 rows x 15 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "olddf=pd.read_excel('Input.xlsx')\n",
    "df['URL']=olddf['URL']\n",
    "df['URL_ID']=olddf['URL_ID']\n",
    "df=df.drop(columns=['text','clean_text','filename','NO. OF SENTENCES'])\n",
    "df\n",
    "df=df[['URL_ID','URL','POSITIVE SCORE','NEGATIVE SCORE','POLARITY SCORE','SUBJECTIVITY SCORE','AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT','SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.555555</td>\n",
       "      <td>0.024862</td>\n",
       "      <td>12.928571</td>\n",
       "      <td>79.005525</td>\n",
       "      <td>36.773639</td>\n",
       "      <td>12.928571</td>\n",
       "      <td>286</td>\n",
       "      <td>362</td>\n",
       "      <td>2.317680</td>\n",
       "      <td>4</td>\n",
       "      <td>6.870166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.038576</td>\n",
       "      <td>11.620690</td>\n",
       "      <td>80.712166</td>\n",
       "      <td>36.933142</td>\n",
       "      <td>11.620690</td>\n",
       "      <td>272</td>\n",
       "      <td>337</td>\n",
       "      <td>2.448071</td>\n",
       "      <td>2</td>\n",
       "      <td>7.062315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>0.049847</td>\n",
       "      <td>12.766234</td>\n",
       "      <td>79.654120</td>\n",
       "      <td>36.968142</td>\n",
       "      <td>12.766234</td>\n",
       "      <td>783</td>\n",
       "      <td>983</td>\n",
       "      <td>2.451679</td>\n",
       "      <td>13</td>\n",
       "      <td>7.080366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>14.933333</td>\n",
       "      <td>81.250000</td>\n",
       "      <td>38.473333</td>\n",
       "      <td>14.933333</td>\n",
       "      <td>182</td>\n",
       "      <td>224</td>\n",
       "      <td>2.531250</td>\n",
       "      <td>1</td>\n",
       "      <td>7.299107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>0.061564</td>\n",
       "      <td>10.016667</td>\n",
       "      <td>74.708819</td>\n",
       "      <td>33.890194</td>\n",
       "      <td>10.016667</td>\n",
       "      <td>449</td>\n",
       "      <td>601</td>\n",
       "      <td>2.381032</td>\n",
       "      <td>28</td>\n",
       "      <td>6.866889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>167.0</td>\n",
       "      <td>https://insights.blackcoffer.com/role-big-data...</td>\n",
       "      <td>13</td>\n",
       "      <td>39</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.073550</td>\n",
       "      <td>10.552239</td>\n",
       "      <td>82.885431</td>\n",
       "      <td>37.375068</td>\n",
       "      <td>10.552239</td>\n",
       "      <td>586</td>\n",
       "      <td>707</td>\n",
       "      <td>2.473833</td>\n",
       "      <td>15</td>\n",
       "      <td>7.152758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>168.0</td>\n",
       "      <td>https://insights.blackcoffer.com/sales-forecas...</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.058288</td>\n",
       "      <td>12.767442</td>\n",
       "      <td>78.142077</td>\n",
       "      <td>36.363807</td>\n",
       "      <td>12.767442</td>\n",
       "      <td>429</td>\n",
       "      <td>549</td>\n",
       "      <td>2.444444</td>\n",
       "      <td>0</td>\n",
       "      <td>7.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>169.0</td>\n",
       "      <td>https://insights.blackcoffer.com/detect-data-e...</td>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>-0.880000</td>\n",
       "      <td>0.089127</td>\n",
       "      <td>8.904762</td>\n",
       "      <td>86.809269</td>\n",
       "      <td>38.285612</td>\n",
       "      <td>8.904762</td>\n",
       "      <td>487</td>\n",
       "      <td>561</td>\n",
       "      <td>2.554367</td>\n",
       "      <td>6</td>\n",
       "      <td>7.140820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>170.0</td>\n",
       "      <td>https://insights.blackcoffer.com/data-exfiltra...</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>0.052239</td>\n",
       "      <td>8.645161</td>\n",
       "      <td>76.119403</td>\n",
       "      <td>33.905826</td>\n",
       "      <td>8.645161</td>\n",
       "      <td>204</td>\n",
       "      <td>268</td>\n",
       "      <td>2.462687</td>\n",
       "      <td>11</td>\n",
       "      <td>7.085821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>171.0</td>\n",
       "      <td>https://insights.blackcoffer.com/impacts-of-co...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.023377</td>\n",
       "      <td>14.259259</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>37.703704</td>\n",
       "      <td>14.259259</td>\n",
       "      <td>308</td>\n",
       "      <td>385</td>\n",
       "      <td>2.358442</td>\n",
       "      <td>1</td>\n",
       "      <td>7.020779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL  \\\n",
       "0       1.0  https://insights.blackcoffer.com/how-is-login-...   \n",
       "1       2.0  https://insights.blackcoffer.com/how-does-ai-h...   \n",
       "2       3.0  https://insights.blackcoffer.com/ai-and-its-im...   \n",
       "3       4.0  https://insights.blackcoffer.com/how-do-deep-l...   \n",
       "4       5.0  https://insights.blackcoffer.com/how-artificia...   \n",
       "..      ...                                                ...   \n",
       "165   167.0  https://insights.blackcoffer.com/role-big-data...   \n",
       "166   168.0  https://insights.blackcoffer.com/sales-forecas...   \n",
       "167   169.0  https://insights.blackcoffer.com/detect-data-e...   \n",
       "168   170.0  https://insights.blackcoffer.com/data-exfiltra...   \n",
       "169   171.0  https://insights.blackcoffer.com/impacts-of-co...   \n",
       "\n",
       "     POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0                 2               7       -0.555555            0.024862   \n",
       "1                 7               6        0.076923            0.038576   \n",
       "2                30              19        0.224490            0.049847   \n",
       "3                 4               1        0.600000            0.022321   \n",
       "4                20              17        0.081081            0.061564   \n",
       "..              ...             ...             ...                 ...   \n",
       "165              13              39       -0.500000            0.073550   \n",
       "166              20              12        0.250000            0.058288   \n",
       "167               3              47       -0.880000            0.089127   \n",
       "168               4              10       -0.428571            0.052239   \n",
       "169               0               9       -1.000000            0.023377   \n",
       "\n",
       "     AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0              12.928571                    79.005525  36.773639   \n",
       "1              11.620690                    80.712166  36.933142   \n",
       "2              12.766234                    79.654120  36.968142   \n",
       "3              14.933333                    81.250000  38.473333   \n",
       "4              10.016667                    74.708819  33.890194   \n",
       "..                   ...                          ...        ...   \n",
       "165            10.552239                    82.885431  37.375068   \n",
       "166            12.767442                    78.142077  36.363807   \n",
       "167             8.904762                    86.809269  38.285612   \n",
       "168             8.645161                    76.119403  33.905826   \n",
       "169            14.259259                    80.000000  37.703704   \n",
       "\n",
       "     AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                           12.928571                 286         362   \n",
       "1                           11.620690                 272         337   \n",
       "2                           12.766234                 783         983   \n",
       "3                           14.933333                 182         224   \n",
       "4                           10.016667                 449         601   \n",
       "..                                ...                 ...         ...   \n",
       "165                         10.552239                 586         707   \n",
       "166                         12.767442                 429         549   \n",
       "167                          8.904762                 487         561   \n",
       "168                          8.645161                 204         268   \n",
       "169                         14.259259                 308         385   \n",
       "\n",
       "     SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0             2.317680                  4         6.870166  \n",
       "1             2.448071                  2         7.062315  \n",
       "2             2.451679                 13         7.080366  \n",
       "3             2.531250                  1         7.299107  \n",
       "4             2.381032                 28         6.866889  \n",
       "..                 ...                ...              ...  \n",
       "165           2.473833                 15         7.152758  \n",
       "166           2.444444                  0         7.222222  \n",
       "167           2.554367                  6         7.140820  \n",
       "168           2.462687                 11         7.085821  \n",
       "169           2.358442                  1         7.020779  \n",
       "\n",
       "[170 rows x 15 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df[['URL_ID','URL','POSITIVE SCORE','NEGATIVE SCORE','POLARITY SCORE','SUBJECTIVITY SCORE','AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT','SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH']]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('Output Data.xlsx')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88bce2c6eb1e59fa01436dcdb966176566e1a61ceea6a4f60ea610a447e1e7ed"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
